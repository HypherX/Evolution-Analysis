# Smaller Language Models Are Better Instruction Evolvers
## Overview
We are the first to analyze the different performance of LLMs (70B) and SLMs (8B) in constructing instructions, and extensive experiments demonstrate that SLMs can construct more complex and diverse instructions compared to LLMs.

## Dependencies
General Setup Environment:
- Python 3.10
- PyTorch 2.4.0 + cu121
- Transformers 4.46.2

Use the following command to install the dependencies.
```
cd Evol-Instruct/
pip3 install -r requirements.txt
```

## Evol-Instruct
- The evolution prompt is described in our paper and can also be found in the `Evol_Instruct/breadth.py` and `Evol_Instruct/depth.py` files.
- All dataset formats follow the structure used in LLaMA-Factory, which consists of JSON files with three keys: `instruction`, `input`, and `output`.
- You can use the following command to evolve instructions.

```
python3 Evol_Instruct/evol_instruct.py \
    --model_name_or_path <Your model path> \
    --source_file <Your source instruction file path> \
    --target_file <The evolved file path> \
    --round_num <0 represents the first round evolution> \
    --temperature 0.7 \
    --max_tokens 2048 \
    --use_breadth <Whether use breadth evolution> \
    --tp <8 represents the tensor parallel numbers>
```

- You can use the following command to generate responses.

```
python3 Evol_Instruct/gen_response.py \
    --model_name_or_path <Your model path> \
    --inst_file <The evolved file path>
```

- You can also run the `Evol_Instruct/eval_complex_rate`, `Evol_Instruct/eval_complex_score`, `Evol_Instruct/eval_reward`, `Evol_Instruct/get_response_prob` and `Evol_Instruct/eval_loss.py` files to evaluate the instructions' complex rate, complex score, instruction rewards, response logprobs, and obtain IFD/IC-IFD score.

- You can run `Evol_Instruct/compare_response.py` to obtain the responses for AlpacaFarm and run `Evol_Instruct/judge.py` to get the win-tie-lose rate for comparison of two fine-tuned models.

## AutoIF
You can run the code in the following order to construct the instruction data. The code is based on the original AutoIF repository, with added sections for model inference using the vLLM framework.

```
python3 AutoIF/1_RFT.py
python3 AutoIF/2_Verification.py
python3 AutoIF/3_Cross_validation.py
python3 AutoIF/4_Concat_ShareGPT.py
```

- The `seed_instruction.txt` file can be downloaded from the original AutoIF repository. We use ShareGPT dataset downloaded from ModelScope, and the link is: https://modelscope.cn/datasets/AI-ModelScope/sharegpt_gpt4
- You can run `AutoIF/eval_diversity.py` to evaluate the diversity of instructions.

## Auto_Evol_Instruct
- The evolution prompt is described in our paper and can also be found in the `Auto_Evol_Instruct/auto_evol_instruct.py` file.
- You can use the following command to automatically evolve instructions.

```
python3 Auto_Evol_Instruct/evol_instruct.py \
    --model_name_or_path <Your model path> \
    --source_file <Your source instruction file path> \
    --target_file <The evolved file path> \
    --round_num <0 represents the first round evolution> \
    --temperature 0.7 \
    --max_tokens 2048 \
    --use_breadth <Whether use breadth evolution> \
    --tp <8 represents the tensor parallel numbers>
```

- You can use the following command to generate responses.

```
python3 Auto_Evol_Instruct/gen_response.py \
    --model_name_or_path <Your model path> \
    --inst_file <The evolved file path>
```

- You can run `Auto_Evol_Instruct/extract_method_list.py` and `Auto_Evol_Instruct/extract_keywords.py` to obtain the keywords from trajectories generated by models. The extracting prompt template can be found in `Auto_Evol_Instruct/extract_prompt.py`.

- Run `Auto_Evol_Instruct/filter_data.py` to obtain the same number of instruction data generated by SLMs and LLMs.

## Figures
The source code for all the figures in the paper can be found in `Plot_figure/` directory.

## Training
We use [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory) for training base models. Thanks for their excellent work.

You can use the following scripts for instruction tuning:
```
torchrun --nproc_per_node=8 src/train.py \
    --model_name_or_path <model_name_or_path> \
    --use_fast_tokenizer \
    --flash_attn fa2 \
    --template <template> \
    --dataset <dataset> \
    --cutoff_len 2048 \
    --do_train True \
    --stage sft \
    --finetuning_type full \
    --preprocessing_num_workers 64 \
    --overwrite_cache True \
    --output_dir <output_path> \
    --logging_steps 1 \
    --save_strategy epoch \
    --save_total_limit 1 \
    --plot_loss True \
    --overwrite_output_dir True \
    --save_only_model True \
    --per_device_train_batch_size 1 \
    --gradient_accumulation_steps 8 \
    --learning_rate 2.0e-5 \
    --num_train_epochs 3.0 \
    --lr_scheduler_type cosine \
    --warmup_ratio 0.03 \
    --bf16 True \
    --tf32 True \
    --ddp_timeout 180000000 \
    --gradient_checkpointing True \
    --deepspeed examples/deepspeed/ds_z3_config.json
```
For more implementations details, please refer to our paper.